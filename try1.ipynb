{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "df = pd.read_excel('dataset.xlsx')\n",
    "df['KOSTENRUBRIEK - declared'] = df['KOSTENRUBRIEK - declared'].str.lower()\n",
    "df['FLC: REDEN VERWERPING'] = df['FLC: REDEN VERWERPING'].str.lower()\n",
    "df['BESCHRIJVING DECLARATIE'] = df['BESCHRIJVING DECLARATIE'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Flags:\n",
      "the cost was declared in the wrong category\n",
      "double declaration\n",
      "please specify the correct invoiced amount\n",
      "costs that are declared too late, can only be reimbursed for 50%\n",
      "purchase not eligible for subsidy\n",
      "please specify the correct supplier\n"
     ]
    }
   ],
   "source": [
    "# Extract unique flags from the \"FLC: REDEN VERWERPING\" column\n",
    "all_flags = df[\"FLC: REDEN VERWERPING\"].str.split('|', expand=True).stack().str.strip().unique()\n",
    "\n",
    "# Print the unique flags\n",
    "print(\"Unique Flags:\")\n",
    "for flag in all_flags:\n",
    "    print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the phrases to check for in the specified column\n",
    "phrases = {\n",
    "    \"FLC: REDEN VERWERPING\": {\n",
    "        \"double declaration\": \"double\",\n",
    "        \"too late\": \"late\",\n",
    "        \"correct invoiced amount\": \"amount\",\n",
    "        \"supplier\": \"supplier\",\n",
    "        \"wrong category\": \"category\",\n",
    "        \"eligible\": \"eligible\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Iterate through the phrases and update the corresponding columns\n",
    "for column_name, conditions in phrases.items():\n",
    "    for phrase, new_column in conditions.items():\n",
    "        df[new_column] = df[column_name].str.contains(phrase, case=False, na=False).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      1\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "489    0\n",
       "490    0\n",
       "491    0\n",
       "492    0\n",
       "493    0\n",
       "Name: pred.double, Length: 494, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['pred.double'] = 0\n",
    "\n",
    "# Check if an entry has the same values in both columns 'REFERENTIE FACTUUR' and 'DATUM FACTUUR - DECLARED'\n",
    "duplicate_entries = df[df.duplicated(['REFERENTIE FACTUUR', 'DATUM FACTUUR - DECLARED'], keep=False)]\n",
    "\n",
    "for idx, group in duplicate_entries.groupby(['REFERENTIE FACTUUR', 'DATUM FACTUUR - DECLARED']):\n",
    "    # Find the earliest value in 'DECLARATIEDATUM (can be assumed to be close to payment date)' for this entry\n",
    "    earliest_date = group['DECLARATIEDATUM (can be assumed to be close to payment date)'].min()\n",
    "    \n",
    "    # Set 'pred.double' to 1 for entries other than the one with the earliest date\n",
    "    df.loc[group.index, 'pred.double'] = (group['DECLARATIEDATUM (can be assumed to be close to payment date)'] != earliest_date).astype(int)\n",
    "\n",
    "df['pred.double']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "489    0\n",
       "490    0\n",
       "491    0\n",
       "492    0\n",
       "493    0\n",
       "Name: pred.late, Length: 494, dtype: int32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the \"DATUM FACTUUR - DECLARED\" column to datetime format\n",
    "df['DATUM FACTUUR - DECLARED'] = pd.to_datetime(df['DATUM FACTUUR - DECLARED'], errors='coerce')\n",
    "\n",
    "# Create the \"latest\" column by moving the month 6 months forward\n",
    "df['latest'] = df['DATUM FACTUUR - DECLARED'] + pd.DateOffset(months=6)\n",
    "\n",
    "# Convert date columns to datetime objects\n",
    "df['latest'] = pd.to_datetime(df['latest'])\n",
    "df['DECLARATIEDATUM (can be assumed to be close to payment date)'] = pd.to_datetime(df['DECLARATIEDATUM (can be assumed to be close to payment date)'])\n",
    "\n",
    "# Calculate the time difference and create a new column 'timediff'\n",
    "df['pred.late'] = (df['DECLARATIEDATUM (can be assumed to be close to payment date)'] - df['latest']).dt.days > 0\n",
    "df['pred.late'] = df['pred.late'].astype(int)\n",
    "\n",
    "df['pred.late']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong amount (only EUR works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "489    0\n",
       "490    0\n",
       "491    0\n",
       "492    0\n",
       "493    0\n",
       "Name: pred.amount, Length: 494, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pred.amount'] = (df['BETAALD BEDRAG - extracted from invoice'] - df['BETAALD BEDRAG - declared ']).apply(lambda x: 1 if x < 0 else 0)\n",
    "df['pred.amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong supplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "489    0\n",
       "490    0\n",
       "491    0\n",
       "492    0\n",
       "493    0\n",
       "Name: pred.supplier, Length: 494, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "se = SnowballStemmer(\"english\")\n",
    "\n",
    "x = df[\"LEVERANCIER - EXTRACTED\"]\n",
    "y = df[\"LEVERANCIER - DECLARED\"]\n",
    "\n",
    "def clean_message(message_list):\n",
    "    ms = []\n",
    "    for word in message_list:\n",
    "        ms.append(se.stem(word))\n",
    "    return ms \n",
    "\n",
    "def lever_equals(x, y):\n",
    "    if x == y:\n",
    "        return 0\n",
    "    x = re.sub(r'[^\\w\\s]', ' ', x)\n",
    "    y = re.sub(r'[^\\w\\s]', ' ', y)\n",
    "    split_x = clean_message(word_tokenize(x.lower()))\n",
    "    split_y = clean_message(word_tokenize(y.lower()))\n",
    "    \n",
    "    if len(split_x) == len(split_y):\n",
    "        for i in range(len(split_x)):\n",
    "            if split_x[i][0] != split_y[i][0]:\n",
    "                return 1\n",
    "        return 0\n",
    "    elif len(split_x)!=0 and len(split_y)!=0:\n",
    "        mini = min(len(split_x), len(split_y))\n",
    "        for i in range(mini):\n",
    "            if split_x[i] != split_y[i]:\n",
    "                return 1\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "# Create a new column 'pred.supplier' in df and store the results\n",
    "df['pred.supplier'] = [lever_equals(x[i], y[i]) for i in range(len(x))]\n",
    "df['pred.supplier']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "\n",
      "Classification Report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                  equipment       0.95      0.95      0.95        19\n",
      "          external services       0.87      0.91      0.89        22\n",
      "infrastructure and building       0.94      0.94      0.94        16\n",
      "          preparation costs       0.90      0.82      0.86        22\n",
      "               travel costs       0.94      1.00      0.97        16\n",
      "\n",
      "                   accuracy                           0.92        95\n",
      "                  macro avg       0.92      0.92      0.92        95\n",
      "               weighted avg       0.92      0.92      0.92        95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Filter rows where \"category\" column has value 0\n",
    "df2 = df[df['category'] == 0].copy()  # Make a copy of the filtered DataFrame\n",
    "\n",
    "# Save the filtered DataFrame to a CSV file\n",
    "df2.to_csv('cat_data.csv', index=False)\n",
    "\n",
    "# Train model on positive targets\n",
    "X = df2['BESCHRIJVING DECLARATIE']\n",
    "y = df2['KOSTENRUBRIEK - declared']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "train_predictions = classifier.predict(X_train_tfidf)\n",
    "\n",
    "# Update 'pred.category' in the original DataFrame for the training set\n",
    "df2.loc[df2['BESCHRIJVING DECLARATIE'].index, 'pred.category'] = (train_predictions != y_train).astype(int)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Update 'pred.category' in the original DataFrame for the test set\n",
    "df2.loc[X_test.index, 'pred.category'] = (test_predictions != y_test).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "classification_rep = classification_report(y_test, test_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5      0.0\n",
       "6      0.0\n",
       "7      0.0\n",
       "8      1.0\n",
       "9      0.0\n",
       "      ... \n",
       "489    0.0\n",
       "490    0.0\n",
       "491    0.0\n",
       "492    0.0\n",
       "493    0.0\n",
       "Name: pred.category, Length: 472, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['pred.category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "\n",
      "Classification Report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                  equipment       0.00      0.00      0.00         8\n",
      "          external services       0.00      0.00      0.00         8\n",
      "infrastructure and building       0.00      0.00      0.00         3\n",
      "          preparation costs       0.25      0.50      0.33         2\n",
      "               travel costs       0.00      0.00      0.00         1\n",
      "\n",
      "                   accuracy                           0.05        22\n",
      "                  macro avg       0.05      0.10      0.07        22\n",
      "               weighted avg       0.02      0.05      0.03        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use model to predict on positive targets\n",
    "df_category_1 = df[df['category'] == 1].copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "\n",
    "# Extract text data and labels\n",
    "X_category_1 = df_category_1['BESCHRIJVING DECLARATIE']\n",
    "y_category_1 = df_category_1['KOSTENRUBRIEK - declared']\n",
    "\n",
    "# Vectorize the text data using the same TF-IDF vectorizer\n",
    "X_category_1_tfidf = vectorizer.transform(X_category_1)\n",
    "\n",
    "# Make predictions on the instances where 'category' has value 1\n",
    "predictions_category_1 = classifier.predict(X_category_1_tfidf)\n",
    "\n",
    "# Update 'pred.category' in the original DataFrame\n",
    "df_category_1['pred.category'] = 1-(predictions_category_1 == y_category_1).astype(int)\n",
    "\n",
    "# Evaluate the model on these instances\n",
    "accuracy_category_1 = accuracy_score(y_category_1, predictions_category_1)\n",
    "classification_rep_category_1 = classification_report(y_category_1, predictions_category_1)\n",
    "\n",
    "print(f'Accuracy: {1 - accuracy_category_1:.2f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_rep_category_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "19     1\n",
       "21     1\n",
       "22     1\n",
       "24     0\n",
       "29     1\n",
       "30     1\n",
       "32     1\n",
       "35     1\n",
       "40     1\n",
       "45     1\n",
       "58     1\n",
       "67     1\n",
       "72     1\n",
       "83     1\n",
       "92     1\n",
       "95     1\n",
       "105    1\n",
       "Name: pred.category, dtype: int32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_category_1['pred.category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.0\n",
       "1      0.0\n",
       "2      0.0\n",
       "3      1.0\n",
       "4      0.0\n",
       "      ... \n",
       "489    1.0\n",
       "490    1.0\n",
       "491    1.0\n",
       "492    1.0\n",
       "493    1.0\n",
       "Name: pred.category, Length: 494, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df2, df_category_1], ignore_index=True)\n",
    "\n",
    "df['pred.category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not eligible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "489    0\n",
       "490    0\n",
       "491    0\n",
       "492    0\n",
       "493    0\n",
       "Name: pred.eligible, Length: 494, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the Excel file\n",
    "df_eli = pd.read_excel('eli_data.xlsx')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_eli['BESCHRIJVING DECLARATIE'], df_eli['eligible'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer, stopwords removal, and Decision Tree classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),  # Using TF-IDF vectorizer with stopwords removal\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
    "\n",
    "# Assuming 'df' is your DataFrame for testing\n",
    "# Make predictions on the 'df' DataFrame\n",
    "df['pred.eligible'] = pipeline.predict(df['BESCHRIJVING DECLARATIE'])\n",
    "\n",
    "df['pred.eligible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Excel file saved to: output_file.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file_path = 'output_file.xlsx'\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated Excel file saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      double       1.00      0.40      0.57         5\n",
      "        late       1.00      0.98      0.99        50\n",
      "      amount       1.00      0.39      0.56        18\n",
      "    supplier       0.62      0.89      0.73         9\n",
      "    category       0.54      0.95      0.69        22\n",
      "    eligible       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       0.80      0.85      0.82       110\n",
      "   macro avg       0.86      0.77      0.76       110\n",
      "weighted avg       0.88      0.85      0.82       110\n",
      " samples avg       0.16      0.16      0.16       110\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2831   23]\n",
      " [  17   93]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camd1\\miniconda3\\envs\\holyhack\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\camd1\\miniconda3\\envs\\holyhack\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\camd1\\miniconda3\\envs\\holyhack\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Extract the actual values\n",
    "y_true = df[['double', 'late', 'amount', 'supplier', 'category', 'eligible']].values\n",
    "\n",
    "# Extract the predicted values\n",
    "y_pred = df[['pred.double', 'pred.late', 'pred.amount', 'pred.supplier', 'pred.category', 'pred.eligible']].values\n",
    "\n",
    "# Calculate classification report\n",
    "classification_rep = classification_report(y_true, y_pred, target_names=['double', 'late', 'amount', 'supplier', 'category', 'eligible'])\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true.flatten(), y_pred.flatten())\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holyhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
